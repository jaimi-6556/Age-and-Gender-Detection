{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f7cfd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename.lower().endswith((\u001b[33m\"\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.jpeg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m     42\u001b[39m     img_path = os.path.join(input_dir, filename)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     label = \u001b[43mauto_label_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m label == \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     46\u001b[39m         shutil.copy(img_path, os.path.join(long_dir, filename))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mauto_label_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_label_image\u001b[39m(image_path):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     img = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Input UTKFace folder\n",
    "input_dir = \"UTKFace\"\n",
    "\n",
    "# Output folders\n",
    "long_dir = \"hair_dataset/long\"\n",
    "short_dir = \"hair_dataset/short\"\n",
    "os.makedirs(long_dir, exist_ok=True)\n",
    "os.makedirs(short_dir, exist_ok=True)\n",
    "\n",
    "# Load Haar face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def auto_label_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None  # Skip if face not detected\n",
    "\n",
    "    (x, y, w, h) = faces[0]\n",
    "    face_bottom = y + h\n",
    "    image_bottom = img.shape[0]\n",
    "    below_face = image_bottom - face_bottom\n",
    "\n",
    "    # Heuristic: long hair if enough space below the face\n",
    "    if below_face > h * 0.8:\n",
    "        return \"long\"\n",
    "    else:\n",
    "        return \"short\"\n",
    "\n",
    "# Go through all images\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        img_path = os.path.join(input_dir, filename)\n",
    "        label = auto_label_image(img_path)\n",
    "\n",
    "        if label == \"long\":\n",
    "            shutil.copy(img_path, os.path.join(long_dir, filename))\n",
    "        elif label == \"short\":\n",
    "            shutil.copy(img_path, os.path.join(short_dir, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663a5839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"hair_length_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6533 images belonging to 2 classes.\n",
      "Found 1633 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaimi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\jaimi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 492ms/step - accuracy: 0.9645 - loss: 0.1723 - val_accuracy: 0.9645 - val_loss: 0.1584\n",
      "Epoch 2/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 249ms/step - accuracy: 0.9645 - loss: 0.1577 - val_accuracy: 0.9645 - val_loss: 0.1572\n",
      "Epoch 3/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 277ms/step - accuracy: 0.9645 - loss: 0.1509 - val_accuracy: 0.9645 - val_loss: 0.1417\n",
      "Epoch 4/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 268ms/step - accuracy: 0.9645 - loss: 0.1423 - val_accuracy: 0.9645 - val_loss: 0.1324\n",
      "Epoch 5/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 242ms/step - accuracy: 0.9645 - loss: 0.1321 - val_accuracy: 0.9645 - val_loss: 0.1284\n",
      "Epoch 6/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 243ms/step - accuracy: 0.9645 - loss: 0.1243 - val_accuracy: 0.9645 - val_loss: 0.1385\n",
      "Epoch 7/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 248ms/step - accuracy: 0.9645 - loss: 0.1175 - val_accuracy: 0.9645 - val_loss: 0.1263\n",
      "Epoch 8/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 246ms/step - accuracy: 0.9645 - loss: 0.1114 - val_accuracy: 0.9645 - val_loss: 0.1323\n",
      "Epoch 9/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 249ms/step - accuracy: 0.9645 - loss: 0.1086 - val_accuracy: 0.9645 - val_loss: 0.1281\n",
      "Epoch 10/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 252ms/step - accuracy: 0.9646 - loss: 0.1046 - val_accuracy: 0.9645 - val_loss: 0.1236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hair length model saved as hair_length_model.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = (128, 128)   # ðŸ‘ˆ Change this to any fixed size you want (e.g. 224,224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "HAIR_DIR = \"hair_dataset\"  # must contain 'short/' and 'long/' subfolders\n",
    "\n",
    "# Image Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    HAIR_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    HAIR_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)\n",
    "\n",
    "# Save model\n",
    "model.save(\"hair_length_model.h5\")\n",
    "print(\"âœ… Hair length model saved as hair_length_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69033dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hair(image_path):\n",
    "    # Load and preprocess\n",
    "    img = load_img(image_path, target_size=IMG_SIZE)\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # (1,128,128,3)\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img_array)[0][0]\n",
    "    label = \"Long Hair\" if prediction > 0.5 else \"Short Hair\"\n",
    "    print(f\"Prediction: {label} ({prediction:.4f})\")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0c3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# After training your model\n",
    "model.save(\"hair_length_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c811ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save without optimizer state (recommended if only for inference)\n",
    "model.save(\"my_model.h5\", include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2722f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
